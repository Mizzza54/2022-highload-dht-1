# Асинхронный HTTP-клиент

На данном этапе HTTP-клиент был переписан на использование асинхронной модели с использованием `CompletableFuture`.
Как мы увидим, это поможет справиться с зависаниями, наблюдаемыми на предыдущем этапе.

## PUT

```text
$ ./wrk -L -c 64 -t 4 -R 7000 -d 60 -s ~/study-files/highload/lua/put.lua http://localhost:19234

Running 1m test @ http://localhost:19234
  4 threads and 64 connections
  Thread calibration: mean lat.: 861.555ms, rate sampling interval: 2787ms
  Thread calibration: mean lat.: 890.568ms, rate sampling interval: 2791ms
  Thread calibration: mean lat.: 385.297ms, rate sampling interval: 1609ms
  Thread calibration: mean lat.: 374.701ms, rate sampling interval: 1594ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     9.68ms   23.43ms 171.90ms   94.48%
    Req/Sec     1.48k   255.44     1.84k    52.08%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    4.15ms
 75.000%    5.51ms
 90.000%    9.39ms
 99.000%  131.71ms
 99.900%  150.53ms
 99.990%  165.12ms
 99.999%  168.06ms
100.000%  172.03ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.222     0.000000            1         1.00
       1.429     0.100000        28382         1.11
       2.197     0.200000        56729         1.25
       2.971     0.300000        85119         1.43
       3.619     0.400000       113460         1.67
       4.147     0.500000       141899         2.00
       4.395     0.550000       156179         2.22
       4.639     0.600000       170217         2.50
       4.899     0.650000       184393         2.86
       5.183     0.700000       198683         3.33
       5.515     0.750000       212765         4.00
       5.727     0.775000       219862         4.44
       5.975     0.800000       226907         5.00
       6.303     0.825000       234020         5.71
       6.779     0.850000       241106         6.67
       7.583     0.875000       248204         8.00
       8.295     0.887500       251745         8.89
       9.391     0.900000       255266        10.00
      11.071     0.912500       258811        11.43
      15.423     0.925000       262358        13.33
      24.207     0.937500       265904        16.00
      31.151     0.943750       267676        17.78
      44.511     0.950000       269448        20.00
#[Mean    =        9.676, StdDeviation   =       23.431]
#[Max     =      171.904, Total count    =       283628]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  347784 requests in 1.00m, 22.22MB read
  Socket errors: connect 0, read 0, write 0, timeout 285
Requests/sec:   5796.02
Transfer/sec:    379.23KB
```

Тут мы видим, что 95-ый перцентиль запросов обрабатывается за 44мс, что довольно приемлемо для сервиса с избыточной
межсерверной репликацией.

Попробуем повысить частоту запросов до 20к:

```text
$ ./wrk -L -c 64 -t 4 -R 20000 -d 20 -s ~/study-files/highload/lua/put.lua http://localhost:19234
Running 20s test @ http://localhost:19234
  4 threads and 64 connections
  Thread calibration: mean lat.: 2888.084ms, rate sampling interval: 10493ms
  Thread calibration: mean lat.: 2844.221ms, rate sampling interval: 10436ms
  Thread calibration: mean lat.: 2908.590ms, rate sampling interval: 10502ms
  Thread calibration: mean lat.: 2901.389ms, rate sampling interval: 10518ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     8.55s     1.52s   11.18s    58.04%
    Req/Sec       -nan      -nan   0.00      0.00%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    8.58s 
 75.000%    9.86s 
 90.000%   10.65s 
 99.000%   11.10s 
 99.900%   11.16s 
 99.990%   11.17s 
 99.999%   11.19s 
100.000%   11.19s 

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

    5779.455     0.000000            1         1.00
    6438.911     0.100000         7995         1.11
    6971.391     0.200000        15906         1.25
    7532.543     0.300000        23895         1.43
    8065.023     0.400000        31841         1.67
    8577.023     0.500000        39788         2.00
    8830.975     0.550000        43826         2.22
    9076.735     0.600000        47790         2.50
    9330.687     0.650000        51792         2.86
    9601.023     0.700000        55723         3.33
    9863.167     0.750000        59734         4.00
   10002.431     0.775000        61682         4.44
   10133.503     0.800000        63732         5.00
   10264.575     0.825000        65670         5.71
   10387.455     0.850000        67653         6.67
   10518.527     0.875000        69691         8.00
   10584.063     0.887500        70655         8.89
   10649.599     0.900000        71703        10.00
   10706.943     0.912500        72660        11.43
   10772.479     0.925000        73648        13.33
   10838.015     0.937500        74652        16.00
   10870.783     0.943750        75166        17.78
   10895.359     0.950000        75581        20.00
#[Mean    =     8547.289, StdDeviation   =     1524.285]
#[Max     =    11182.080, Total count    =        79527]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  160047 requests in 20.00s, 10.23MB read
  Socket errors: connect 0, read 0, write 0, timeout 45
Requests/sec:   8002.21
Transfer/sec:    523.58KB
```

Всё стало резко очень плохо. Ниже разберём почему.

## GET

```text
$ ./wrk -L -c 64 -t 4 -R 7000 -d 60 -s ~/study-files/highload/lua/get.lua http://localhost:19234
Running 1m test @ http://localhost:19234
  4 threads and 64 connections
  Thread calibration: mean lat.: 106.736ms, rate sampling interval: 714ms
  Thread calibration: mean lat.: 105.199ms, rate sampling interval: 721ms
  Thread calibration: mean lat.: 103.243ms, rate sampling interval: 708ms
  Thread calibration: mean lat.: 106.044ms, rate sampling interval: 716ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    11.93ms   29.20ms 206.46ms   90.15%
    Req/Sec     1.50k   135.67     1.76k    63.90%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    2.21ms
 75.000%    3.61ms
 90.000%   38.81ms
 99.000%  144.38ms
 99.900%  181.38ms
 99.990%  197.76ms
 99.999%  203.26ms
100.000%  206.59ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.199     0.000000            1         1.00
       1.055     0.100000        29911         1.11
       1.369     0.200000        59858         1.25
       1.635     0.300000        89734         1.43
       1.905     0.400000       119601         1.67
       2.207     0.500000       149450         2.00
       2.385     0.550000       164431         2.22
       2.587     0.600000       179354         2.50
       2.833     0.650000       194363         2.86
       3.151     0.700000       209287         3.33
       3.607     0.750000       224163         4.00
       3.945     0.775000       231665         4.44
       4.467     0.800000       239123         5.00
       5.347     0.825000       246587         5.71
       7.023     0.850000       254059         6.67
      11.551     0.875000       261521         8.00
      21.103     0.887500       265258         8.89
      38.815     0.900000       268996        10.00
      54.239     0.912500       272730        11.43
      65.535     0.925000       276465        13.33
      75.647     0.937500       280202        16.00
      81.215     0.943750       282086        17.78
      86.783     0.950000       283953        20.00
#[Mean    =       11.927, StdDeviation   =       29.200]
#[Max     =      206.464, Total count    =       298880]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  368346 requests in 1.00m, 274.00MB read
  Socket errors: connect 0, read 0, write 0, timeout 211
Requests/sec:   6139.33
Transfer/sec:      4.57MB
```

95-ый перцентиль у `GET`-запроса, ожидаемо, хуже, но даже 80мс для нас будет ок.

## Flamegraph

Попробуем разобраться, где находится в боттлнек. В описании задания был сделан акцент на профилировке локов, будто
там есть какой-то подвох. Так и оказалось!

Проанализировав flamegraph, мы видим, что:
* В профиле аллокаций нет ничего интересного
* Если посмотреть в профиль CPU, то теперь видно, что мы почти не стоим в сисколлы, а в БЛОКИРОВКИ
* * Это какие-то очень странные методы в Java `HttpClient`, которые под блокировками что-то делают с сокетами,
ставят мониторы на непонятные объекты
* Профиль блокировок показывает, что проблема в неких `HttpClientImpl$SelectorManager` и `ConnectionPool`

# Выводы

* HTTP-клиент из JDK при асинхронной работе инкапсулирует в себе сложные механизмы с блокировками, селекторами,
что упрощает написание кода, но вызывает серьёзный contention.
* Асинхронная реализация клиента помогла справиться с ошибками и таймаутами, но в своём текущем состоянии не способна
обслуживать >10k RPS
* * Возможно, это получится улучшить, как-то залезая в реализацию `HttpClient`, но моих компетенций в Java-разработке
пока для этого недостаточно. Возможно, это спойлер к следующему этапу, who knows.