# Репликация

На данном этапе было реализовано горизонтальное масштабирование через поддержку кластерных конфигураций, состоящих из нескольких узлов,
взаимодействующих друг с другом через реализованный HTTP API.

Также на данном этапе была реализована очередь для перепосылки запросов, что помогает при зависании одного из шардов.

## PUT

Тесты проводились с параметрами `from=3` и `ack=2`

```text
$ ./wrk -c 64 -t 4 -R 10000 -d 60 -s ~/study-files/highload/lua/put.lua http://localhost:19234

Running 1m test @ http://localhost:19234
  4 threads and 64 connections
  Thread calibration: mean lat.: 24.978ms, rate sampling interval: 144ms
  Thread calibration: mean lat.: 25.764ms, rate sampling interval: 147ms
  Thread calibration: mean lat.: 24.533ms, rate sampling interval: 144ms
  Thread calibration: mean lat.: 25.426ms, rate sampling interval: 143ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    27.79ms   42.65ms 496.13ms   88.29%
    Req/Sec     2.51k   219.14     3.38k    70.27%
  599357 requests in 1.00m, 47.54MB read
  Non-2xx or 3xx responses: 391632
Requests/sec:   9989.39
Transfer/sec:    811.43KB
```

Из-за проблем с переполнением очереди мы просто начали выкидывать запросы, которые не успеваем обслужить. С одной
стороны, теперь сервис лучше переживает зависание шардов, с другой -- стало гораздо легче организовать DoS, поскольку
забить очередь запросов тривиально.

## GET

Тесты проводились с параметрами `from=3` и `ack=2`

```text
$ ./wrk -c 64 -t 4 -R 10000 -d 60 -s ~/study-files/highload/lua/get.lua http://localhost:19234

Running 1m test @ http://localhost:19234
  4 threads and 64 connections
  Thread calibration: mean lat.: 19.434ms, rate sampling interval: 94ms
  Thread calibration: mean lat.: 18.390ms, rate sampling interval: 83ms
  Thread calibration: mean lat.: 12.366ms, rate sampling interval: 76ms
  Thread calibration: mean lat.: 18.227ms, rate sampling interval: 80ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    16.86ms   29.27ms 405.76ms   88.95%
    Req/Sec     2.51k   251.06     3.63k    71.55%
  596977 requests in 1.00m, 121.85MB read
  Non-2xx or 3xx responses: 486232
Requests/sec:   9949.98
Transfer/sec:      2.03MB
```

Аналогичная ситуация с GET-запросами. При низком RPS (в районе 1-5k) кластер успешно справляется с нагрузкой, но при её
повышении полностью ложится.

## Flamegraph
Если проанализировать flamegraph, то можно сказать следующее:
* ~60% процессорного времени мы тратим на запрос данных у других шардов, ещё 15% -- на передачу ответа обратно
* * Причём ~30% процессорного времени уходит на установление и закрытие TCP-соединений.
* Никаких аномалий в профиле аллокаций не обнаружено.
* В профиле блокировок очень странный contention на selector-трэде, но его я никак объяснить не могу

## Итоги

Итого, ценой повышения надёжности отказоустойчивости, пришлось пожертвовать пропускной способностью сервиса.
Небольшие размышления, как можно было бы улучшить ситуацию:

* Не использовать HTTP 1.1, поскольку по стандарту мы должны отвечать на запросы в рамках одного соединения в
FIFO-порядке. Это не позволяет нам выборочно отвечать на какие-то запросы (при переполнении очереди), или хоть как-то
их переупорядочивать.
* Как было упомянуто, на каждый проксирующий запрос сейчас создаётся по кратко-живущему HTTP-соединению, что накладывает
ощутимый оверхед. Однако, если этого не делать, то соединения начинают "виснуть" или переходить в какое-то странное,
некорректное состояние. Нашёл только такой способ решения, и работает он плохо.
* Логирование -- далеко не дешёвая операция, поскольку оно может выступать в роли дополнительной точки синхронизации
(внутри себя использует `ReentrantLock`).