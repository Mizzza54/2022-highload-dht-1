# Range-запросы

На данном этапе было необходимо реализовать поддержку range-запросов с поддержкой
[Chunked transfer encoding](https://en.wikipedia.org/wiki/Chunked_transfer_encoding) из стандарта HTTP/1.1. Поскольку
используемое KV-хранилище уже имеет данный интерфейс, реализация обработчика была бы довольно тривиальна. Однако,
поскольку объём данных может быть безумно большим, ответ на запрос кодируется блоками, по блоку на каждый запрос.

При первичном тестировании оказалось, что генерация ключей вида `printf("keyNumber%d", id)` абсолютно не тестируема, и
почти на каждый стресс-тест шлётся порядка 8 ГБ с RPS ~50 :(. Поэтому, скрипт наполнения БД был изменён на следующий:

```lua
counter = 0

request = function()
    path = "/v0/entity?id=keyNumber" .. string.format("%010d", counter)
    wrk.method = "PUT"
    wrk.body = string.rep("Pneumonoultramicroscopicsilicovolcanoconiosis!", 15)
    counter = counter + 1
    return wrk.format("PUT", path)
end
```

То есть, теперь мы добиваем номера ведущими нулями.

## Range длины 1

```text
$ ./wrk -L -c 64 -t 4 -R 10000 -d 30 -s ~/study-files/highload/lua/getMulti.lua http://localhost:19234
Running 30s test @ http://localhost:19234
  4 threads and 64 connections
  Thread calibration: mean lat.: 2.438ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 2.484ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 2.565ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 2.445ms, rate sampling interval: 10ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     0.88ms  533.13us  20.11ms   78.87%
    Req/Sec     2.63k   210.11     6.33k    68.04%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%  847.00us
 75.000%    1.17ms
 90.000%    1.40ms
 99.000%    2.06ms
 99.900%    5.83ms
 99.990%   14.49ms
 99.999%   19.49ms
100.000%   20.13ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.063     0.000000            1         1.00
       0.319     0.100000        19994         1.11
       0.462     0.200000        39847         1.25
       0.591     0.300000        59831         1.43
       0.719     0.400000        79719         1.67
       0.847     0.500000        99621         2.00
       0.911     0.550000       109574         2.22
       0.977     0.600000       119567         2.50
       1.041     0.650000       129580         2.86
       1.105     0.700000       139440         3.33
       1.169     0.750000       149429         4.00
       1.202     0.775000       154389         4.44
       1.235     0.800000       159416         5.00
       1.269     0.825000       164369         5.71
       1.306     0.850000       169386         6.67
       1.348     0.875000       174370         8.00
       1.372     0.887500       176888         8.89
       1.399     0.900000       179326        10.00
       1.430     0.912500       181834        11.43
       1.464     0.925000       184295        13.33
       1.508     0.937500       186788        16.00
       1.535     0.943750       188009        17.78
       1.563     0.950000       189233        20.00
       1.599     0.956250       190503        22.86
       1.640     0.962500       191740        26.67
       1.692     0.968750       192978        32.00
       1.725     0.971875       193600        35.56
       1.761     0.975000       194211        40.00
#[Mean    =        0.878, StdDeviation   =        0.533]
#[Max     =       20.112, Total count    =       199186]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  299635 requests in 30.00s, 228.03MB read
Requests/sec:   9988.31
Transfer/sec:      7.60MB
```

Ожидаемо, для подотрезка длины 1 всё работает молниеносно.

## ## Range длины 10
```text
$ ./wrk -L -c 64 -t 4 -R 10000 -d 30 -s ~/study-files/highload/lua/getMulti.lua http://localhost:19234
Running 30s test @ http://localhost:19234
  4 threads and 64 connections
  Thread calibration: mean lat.: 0.979ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 0.976ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 0.969ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 0.972ms, rate sampling interval: 10ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     0.99ms  478.43us  10.90ms   68.33%
    Req/Sec     2.64k   181.50     4.44k    72.10%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    0.97ms
 75.000%    1.30ms
 90.000%    1.57ms
 99.000%    2.15ms
 99.900%    3.70ms
 99.990%    7.09ms
 99.999%    9.83ms
100.000%   10.91ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.093     0.000000            1         1.00
       0.390     0.100000        19972         1.11
       0.555     0.200000        39911         1.25
       0.701     0.300000        59882         1.43
       0.837     0.400000        79802         1.67
       0.970     0.500000        99652         2.00
       1.035     0.550000       109570         2.22
       1.101     0.600000       119586         2.50
       1.168     0.650000       129609         2.86
       1.235     0.700000       139467         3.33
       1.303     0.750000       149439         4.00
       1.338     0.775000       154390         4.44
       1.374     0.800000       159363         5.00
       1.415     0.825000       164424         5.71
       1.459     0.850000       169329         6.67
       1.512     0.875000       174378         8.00
       1.541     0.887500       176843         8.89
       1.573     0.900000       179337        10.00
       1.609     0.912500       181811        11.43
       1.650     0.925000       184266        13.33
       1.699     0.937500       186789        16.00
       1.725     0.943750       188010        17.78
       1.755     0.950000       189263        20.00
       1.787     0.956250       190495        22.86
       1.826     0.962500       191755        26.67
       1.868     0.968750       192990        32.00
       1.892     0.971875       193605        35.56
       1.918     0.975000       194238        40.00
#[Mean    =        0.990, StdDeviation   =        0.478]
#[Max     =       10.904, Total count    =       199203]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  299649 requests in 30.00s, 2.04GB read
Requests/sec:   9988.12
Transfer/sec:     69.50MB
```

Сервер всё ещё справляется, продолжим увеличивать размер подотрезка.

```text
$ ./wrk -L -c 64 -t 4 -R 10000 -d 30 -s ~/study-files/highload/lua/getMulti.lua http://localhost:19234
Running 30s test @ http://localhost:19234
  4 threads and 64 connections
  Thread calibration: mean lat.: 1125.738ms, rate sampling interval: 4542ms
  Thread calibration: mean lat.: 1063.211ms, rate sampling interval: 4255ms
  Thread calibration: mean lat.: 1074.862ms, rate sampling interval: 4329ms
  Thread calibration: mean lat.: 1093.635ms, rate sampling interval: 4370ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     6.40s     2.74s   11.57s    55.37%
    Req/Sec     1.39k   172.90     1.70k    75.00%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    6.16s 
 75.000%    8.85s 
 90.000%   10.38s 
 99.000%   11.36s 
 99.900%   11.52s 
 99.990%   11.56s 
 99.999%   11.58s 
100.000%   11.58s 

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

    2418.687     0.000000            2         1.00
    2994.175     0.100000        10992         1.11
    3516.415     0.200000        21946         1.25
    4087.807     0.300000        32926         1.43
    5074.943     0.400000        43881         1.67
    6160.383     0.500000        54823         2.00
    6684.671     0.550000        60317         2.22
    7213.055     0.600000        65805         2.50
    7741.439     0.650000        71282         2.86
    8310.783     0.700000        76764         3.33
    8847.359     0.750000        82258         4.00
    9125.887     0.775000        85026         4.44
    9388.031     0.800000        87768         5.00
    9625.599     0.825000        90512         5.71
    9879.551     0.850000        93236         6.67
   10133.503     0.875000        95969         8.00
   10256.383     0.887500        97322         8.89
   10379.263     0.900000        98679        10.00
   10510.335     0.912500       100035        11.43
   10633.215     0.925000       101403        13.33
   10764.287     0.937500       102796        16.00
   10838.015     0.943750       103509        17.78
   10903.551     0.950000       104157        20.00
   10969.087     0.956250       104825        22.86
   11042.815     0.962500       105551        26.67
   11108.351     0.968750       106201        32.00
   11149.311     0.971875       106575        35.56
   11190.271     0.975000       106961        40.00
#[Mean    =     6399.859, StdDeviation   =     2742.794]
#[Max     =    11567.104, Total count    =       109620]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  185089 requests in 30.00s, 12.46GB read
Requests/sec:   6169.18
Transfer/sec:    425.24MB
```

Сервер помер. Хоть размер подотрезка и, казалось бы, очень мал, мы эффективно повысили RPS в 100 раз, с 10k до 1kk.

## Анализ FlameGraph

Поскольку шардирование не применяется на данном этапе, мы успешно тестируем лишь код обработчика запроса, без походов
в сеть и аггрегации.

* Профиль блокировок содержит eps записей, что ожидаемо, т.к. чемпионом по contention был HTTP-клиент. 
* В профиле аллокаций стало больше ByteBuffers, так как теперь мы шлём ответ неизвестной длины кусочками
* В профиле CPU стоим в сисколл `sendto`

## Выводы

* При тестировании очень легко опечататься в названии параметров, и сервер будет исполнять, что ему прислали, а не что
на самом деле хотел пользователь.
* * Например, опечатка (случайная или нарочно) может привести к вычитыванию всей БД. На продакшене лучше применять
ограничитель на размер подотрезка, чтобы одним запросом не убить БД.
* HTTP/1.1 хоть и является текстовым и неэффективным, но вполне успешно справляется с оптимизацией разбиения ответа
на более мелкие части
* * Range-запросы позволили стабильно отдавать до ~100k ключей в секунду, что было невозможно в старой реализации.
* Сервер всё ещё стоит в сисколлы и это очень грустно
