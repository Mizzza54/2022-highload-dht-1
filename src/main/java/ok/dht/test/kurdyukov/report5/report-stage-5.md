# Отчет

## Введение

На этом этапе основные изменения были в [HttpShardServer](./../http/HttpShardServer.java).
Как раз была сделана оптимизация обозначена мной в прошлом этапе.
При асинхронном *multicast* ждать первые ack - ответов, а не form - запросов.
С небольшими поправками к прошлому этапу. Описано чуть-чуть пониже.
Как небольшой спойлер - это дало сильный буст в производительности.
Также был вычищен ``Stream Api``.

## Описание работы кластера.

Итак, теперь, как отмечено выше, я не блокирую основной поток обработки запросов клиента.
Вся обработка теперь перенеслась в большой callback у `Future`. Я собираю там количество успешных ack'ов, если их
количество набралось тому, что пришло в запросе клиента. То я отменяю остальные form - ack задачи. Стоит отметить, что
никаких блокировок нет, используется только локфри.

Обработка GET осуществляется CAS'ом стандартным трюком `while (true) { if(CAS) break; }`.
Мы стараемся обновить результат с наибольшим Timestamp'ом. Иначе брейк. Есть исход получения более актуального ответа
под номером ack + 1. Но это не идет в разрез со спецификацией.

Берутся снепшоты текущего количество ack'ов и количество всего обработанных запросов. В итоге здесь происходит
синхронизация между потоками.

Политика реплицирования осталась та же.

## Ход работы

При нагрузочном тестировании использовался кластер из трех нод. Параметры для реплицирования были следующими ack = 2 и
from = 3.

Результаты обстрела PUT:

````
wrk2 -d 2m -c 64 -t 64 -s put_http_query.lua -R 5000 http://localhost:4242
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    30.76s    15.01s    1.04m    59.21%
    Req/Sec    39.39      4.80    48.00     76.17%
  294323 requests in 2.00m, 18.81MB read
  Non-2xx or 3xx responses: 152
Requests/sec:   2452.38
Transfer/sec:    160.47KB
````

В 64 коннекта и 64 потоков, кластер не выдержал нагрузки в 5000 рпс.

```
wrk2 -d 40s -c 64 -t 64 -s put_http_query.lua -R 1500 http://localhost:4242

  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    28.31ms   73.73ms 623.10ms   92.88%
    Req/Sec    22.92      2.09    34.00     91.21%
  60061 requests in 40.02s, 3.84MB read
  Non-2xx or 3xx responses: 69
Requests/sec:   1500.62
Transfer/sec:     98.21KB
```

Примерно около 1500 рпс можно назвать текущей стабильной нагрузкой нашего кластера.

Сейчас нагрузка происходит в 64 конекта в отличие от предыдущего стейджа, где максимальное количество коннекшенов,
которые давали стабильную нагрузку, было 6 (собственно и в коде я открывал 6 сокетов). На прошлом этапе наш кластер
снова стал синхронный. Потому что мы с легкостью забивали тредпул, блокируя основной поток обработки. И при количестве
подключений свыше 6, половина запросов начинало возвращать 500, связи с режектом задачи, когда мы ее пытаемся скормить
обработчику.

Сейчас же асинхронность вернулась в полной мере. Кластер снова стал держать 64 подключения.

К слову, после прогрева на 1500 рпс кластер стал отвечать еще быстрее.
Этот результат был зафиксирован, когда я начал профилировать.

```
kurdyukov-kir@i109817004 lua % wrk2 -d 1m -c 64 -t 64 -s put_http_query.lua -R 1500 http://localhost:4242
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     8.71ms   42.51ms 565.76ms   96.87%
    Req/Sec    23.24     11.61   187.00     82.66%
  90048 requests in 1.00m, 5.75MB read
  Non-2xx or 3xx responses: 61
Requests/sec:   1500.47
Transfer/sec:     98.19KB
```

Результаты обстрела GET:

```
kurdyukov-kir@i109817004 lua % wrk2 -d 1m -c 64 -t 64 -s get_http_query.lua -R 1500 http://localhost:4242
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     2.29ms    5.13ms 213.63ms   97.34%
    Req/Sec    24.97     43.86   111.00     75.43%
  90058 requests in 1.00m, 65.94MB read
  Non-2xx or 3xx responses: 9
Requests/sec:   1500.34
Transfer/sec:      1.10MB
```

GET без каких либо вопросов выдерживает 1500 рпс, в отличие от PUT, который не сразу стал показывать стабильные
результаты.

```
kurdyukov-kir@i109817004 lua % wrk2 -d 1m -c 64 -t 64 -s get_http_query.lua -R 2500 http://localhost:4242
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     3.24ms    4.40ms 117.31ms   96.19%
    Req/Sec    41.41     49.86   333.00     65.66%
  150080 requests in 1.00m, 109.90MB read
  Non-2xx or 3xx responses: 1
Requests/sec:   2500.96
Transfer/sec:      1.83MB
```

C нагрузкой в 5000 рпс на GET кластер, к сожалению не справился.

```
kurdyukov-kir@i109817004 lua % wrk2 -d 1m -c 64 -t 64 -s get_http_query.lua -R 5000 http://localhost:4242
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    14.88s     7.02s   29.59s    59.76%
    Req/Sec    39.35      4.84    48.00     75.59%
  156643 requests in 1.00m, 114.69MB read
  Non-2xx or 3xx responses: 32
Requests/sec:   2610.62
Transfer/sec:      1.91MB
```

Тем не менее факт на лицо GET показывает результаты лучше PUT.
Гипотеза, объясняющая эту особенность будет описана позже.

Посмотрим результаты профилирования перед тем как сформулировать основные выводы текущего этапа. В целом PUT от GET мало
чем отличается в плане узких мест. Результаты очень схожи.

LOCK: [get_lock](./profiler/png/get_lock.png) - GET, [put_lock](./profiler/png/put_lock.png) - PUT.
Там и там картина схожа. Блокировки LevelDB катастрофически малы по сравнению с сервером кластера.
1 / 3 уходит на callback'и фьюч. 2 / 3 на экзекьютер. - это важное наблюдение нам потребуется.

CPU: [get_cpu](./profiler/png/get_cpu.png) - GET, [put_cpu](./profiler/png/put_cpu.png) - PUT.
Траты LevelDB совсем минимальны. Все cpu уходит на потоки executor'a. И callback'и используется ForkJoinPool.
Парковок стало меньше в тредпуле, потому что теперь потоки не блокируются, а стараются обработать следующий запрос.
Также парковки появились в примитивах синхронизации, что справедливо. А также аккуратно размазались по ForkJoinPool.
Явного узкого места (если не брать Java - HttpClient'a) нету.

ALLOC: [get_alloc](./profiler/png/get_alloc.png) - GET, [put_alloc](./profiler/png/put_alloc.png) - PUT.
Видно как ForkJoinPool аллоцирует вновь создавшийся поток. А также как много съедает аллокаций java - client.
Полностью вытеснив здесь не только LevelDB, но и one - nio.

## Выводы

Первый вопрос, который возник в ходе выполнения данного этапа: почему GET стал на порядок быстрее PUT?
Моя гипотеза в следующем:

По результатам профилирования видно, что БД очень мало есть ресурсов в сравнении с HttpClient'ом и так далее.
Этот парадокс начал возникать с 4 этапа. И тут возникает закономерный вопрос, что если сериализация сильно дороже
десериализации? Ведь в PUT мы сериализуем потенциально form раз. А в GET стараемся десериализовать ack раз. Возможно
здесь может возникнуть узкое место.

Также в GET десериализация асинхронно происходит на ноде, которая осуществляла multicast.
А в PUT, каждая нода сериализует.

В PUT Http запрос чуть-чуть потяжелее, так как в хедер кладеться еще timestamp.

По оптимизации, нужно явно что - то делать с клиентом, по-которому осуществляется общение внутри кластера.
Это узкое место всего кластера. Уж больно много он употребляет ресурсов.

Возможное решение более низкий протокол.

В целом после проведения двух оптимизаций - устранение `Stream Api`, асинхронная обработка c ответом если набралось ack
успешных ответов от нод кластера. Кластер стал гораздо устойчивее к нагрузкам. Снова можно нагружать в количество
соединений большее чем сокетов. Дальнейшей оптимизацией может быть более хитрая работа с обработкой сообщений от
соседних node.

Можно так же заифать момент, что нода шлет сама себе запрос, но так как это асинхронно, то сильно не играет большой
роли. Зато имеем более читаемый код.

Кластер как-никак способен горизонтально масштабироваться. Здесь лучше подумать об таком улучшении как отказа
устойчивость.
Ведь если один узел на долго уйдет в своп, данные разойдутся с остальными репликами. Решением может быть GOSIP проток, а
также алгоритмы консенсуса Paxos или Raft. Но здесь нужно решить, какие гарантии мы предоставляем и какой рпс готовы
держать.